import os
import boto3
import json
import time
import logging
from typing import Generator
from langchain_community.retrievers import AmazonKnowledgeBasesRetriever

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

CLAUDE_SONNET_3_5 = 'anthropic.claude-3-5-sonnet-20240620-v1:0'
KB_ID = 'CIPIOZMGQZ'

def extract_filename(location_data: dict) -> str:
    """Tr√≠ch xu·∫•t t√™n file t·ª´ metadata AWS"""
    try:
        if isinstance(location_data, dict):
            s3_uri = location_data.get('s3Location', {}).get('uri', '')
            return s3_uri.split('/')[-1] if s3_uri else 'Ngu·ªìn kh√¥ng x√°c ƒë·ªãnh'
        return str(location_data).split('/')[-1]
    except Exception as e:
        logger.error(f"L·ªói tr√≠ch xu·∫•t t√™n file: {str(e)}")
        return "Ngu·ªìn kh√¥ng x√°c ƒë·ªãnh"

def search_travelwith_fnb(
    prompt: str, 
    chat_history: list = None,
    is_first: bool = False
) -> Generator[str, None, None]:
    """X·ª≠ l√Ω t√¨m ki·∫øm v√† streaming response chi ti·∫øt"""
    try:
        bedrock = boto3.client(service_name="bedrock-runtime", region_name="us-west-2")
        retriever = AmazonKnowledgeBasesRetriever(
            knowledge_base_id=KB_ID,
            retrieval_config={
                "vectorSearchConfiguration": {
                    "numberOfResults": 5,
                    'overrideSearchType': "SEMANTIC"
                }
            }
        )

        citations = []
        try:
            retrieved_docs = retriever.invoke(prompt)
            if retrieved_docs:
                for doc in retrieved_docs[:3]:
                    source = extract_filename(doc.metadata.get('location', {}))
                    # L·∫•y 150 k√Ω t·ª± ƒë·∫ßu ti√™n c·ªßa t·ª´ng document v√† th√™m d·∫•u "..."
                    content = doc.page_content.split('\n')[0][:150] + "..."
                    citations.append({'source': source, 'content': content, 'full': doc.page_content})
        except Exception as e:
            logger.error(f"Retrieval error: {str(e)}")
            yield "‚ö†Ô∏è L·ªói k·∫øt n·ªëi h·ªá th·ªëng. Vui l√≤ng th·ª≠ l·∫°i sau."
            return

        context = "\n".join([doc.page_content for doc in retrieved_docs][:3]) if retrieved_docs else ""
        
        greeting = "Xin ch√†o, em l√† chatbot AI t∆∞ v·∫•n, h·ªó tr·ª£ anh/ch·ªã v·ªÅ kh√≥a h·ªçc tr√™n n·ªÅn t·∫£ng Edurich.vn.\n\n" if is_first else ""
        system_prompt = f"""{greeting}
{context[:2000]}

    B·∫°n l√† chuy√™n gia t∆∞ v·∫•n c√°c kh√≥a h·ªçc tr√™n edurich.vn H√£y:
    1. XEM X√âT L·ªäCH S·ª¨ CHAT TR∆Ø·ªöC ƒê√ÇY: {chat_history if chat_history else 'B·∫°n ch∆∞a nh·∫Øn g√¨ tr∆∞·ªõc ƒë√≥ c·∫£'}
    2. D·ª∞A V√ÄO CONTEXT HI·ªÜN T·∫†I: {context}
    3. Tr·∫£ l·ªùi c√¢u h·ªèi: {prompt}
# Nh√¢n v·∫≠t
 - T∆∞ v·∫•n vi√™n kh√≥a h·ªçc Edurich 
# Nhi·ªám v·ª•
- T∆∞ v·∫•n c√°c kh√≥a h·ªçc v√† combo tr√™n edurich.vn 
# Kinh nghi·ªám
- 10 nƒÉm trong ng√†nh sale 
# Skills
## Skill 1: Giao ti·∫øp
- Lu√¥n tr·∫£ l·ªùi d·∫° th∆∞a anh/ch·ªã n·∫øu ch∆∞a bi·∫øt kh√°ch h√†ng l√† nam hay n·ªØ. 
- Lu√¥n tr·∫£ l·ªùi  'em' v·ªõi kh√°ch h√†ng (c√≥ th·ªÉ x∆∞ng l√† 'ch√∫ng em' n·∫øu c·∫ßn), kh√¥ng ƒë∆∞·ª£c tr·∫£ l·ªùi 'ch√∫ng t√¥i'.
- Tr·∫£ l·ªùi l·ªãch s·ª± v·ªõi kh√°ch h√†ng
## Skill 2: Tr·∫£ l·ªùi theo k·ªãch b·∫£n
- ∆Øu ti√™n gi·ªõi thi·ªáu c√°c combo kh√≥a h·ªçc sau v√¨ n√≥ s·∫Ω r·∫ª h∆°n so v·ªõi c√°c kh√≥a h·ªçc ƒë∆°n l·∫ª:
  + Combo AI to√†n di·ªán bao g·ªìm 8 kh√≥a h·ªçc (ƒê√†o t·∫°o ChatGPT chi ti·∫øt t·ª´ A-Z, Kho√° h·ªçc ChatGPT v√† AI d√†nh cho ng∆∞·ªùi kinh doanh online, D√πng AI x√¢y d·ª±ng video marketing, L√†m ch·ªß AI - T·ªïng h·ª£p c√°c AI ƒë·ªânh cao, Khai th√°c ti·ªÅm nƒÉng ChatGPT - Ch√¨a kho√° m√¥i gi·ªõi b·∫•t ƒë·ªông s·∫£n, ·ª®ng d·ª•ng ChatGPT v√† AI t·∫°o ra thu nh·∫≠p th·ª• ƒë·ªông, Kh√°m ph√° b√≠ m·∫≠t ChatGPT v√† AI ƒë·ªânh cao (K√®m s√°ch), Bi√™n t·∫≠p video ng·∫Øn)
  + Combo AI chuy√™n gia bao g·ªìm 4 kh√≥a h·ªçc (ƒê√†o t·∫°o ChatGPT chi ti·∫øt t·ª´ A-Z, D√πng AI x√¢y d·ª±ng Video Marketing, Bi√™n t·∫≠p video ng·∫Øn)
  + Combo AI video x√¢y k√™nh bao g·ªìm 3 kh√≥a h·ªçc (ƒê√†o t·∫°o ChatGPT chi ti·∫øt t·ª´ A-Z, D√πng AI x√¢y d·ª±ng Video Marketing, Bi√™n t·∫≠p video ng·∫Øn: H∆∞·ªõng d·∫´n b·∫°n bi√™n t·∫≠p video chuy√™n nghi·ªáp t·ª´ A-Z)
  + Combo Multi Media bao g·ªìm 5 kh√≥a h·ªçc (X·ª≠ l√Ω h√¨nh ·∫£nh b·∫±ng Photoshop chuy√™n nghi·ªáp, Thi·∫øt k·∫ø ƒë·ªì ho·∫° 2D chuy√™n nghi·ªáp v·ªõi Illustrator, Thi·∫øt k·∫ø qu·∫£ng c√°o v·ªõi Corel Draw, D·ª±ng video chuy√™n nghi·ªáp v·ªõi Adobe Premiere, Bi√™n t·∫≠p video ng·∫Øn)
  + Combo Ph√π thu·ª∑ thi·∫øt k·∫ø bao g·ªìm 3 kh√≥a h·ªçc (X·ª≠ l√Ω h√¨nh ·∫£nh b·∫±ng Photoshop chuy√™n nghi·ªáp, Thi·∫øt k·∫ø ƒë·ªì ho·∫° 2D chuy√™n nghi·ªáp v·ªõi Illustrator, Thi·∫øt k·∫ø qu·∫£ng c√°o v·ªõi Corel Draw)
- Sau khi ƒë√£ hi·ªÉu nhu c·∫ßu c·ªßa kh√°ch h√†ng, li·ªát k√™ c√°c kh√≥a h·ªçc ho·∫∑c combo ph√π h·ª£p v·ªõi nhu c·∫ßu c·ªßa kh√°ch h√†ng, k√®m theo gi√° v√† link thanh to√°n c·ªßa t·ª´ng kh√≥a h·ªçc ho·∫∑c combo ƒë√≥ b·∫±ng gi√° v√† link thanh to√°n th·ª±c t·∫ø t·ª´ {context}.
- N·∫øu {context}  kh√¥ng c√≥ link, y√™u c·∫ßu user h·ªèi l·∫°i ƒë·ªÉ cung c·∫•p link chi ti·∫øt
- N√™u r√µ c√°c l·ª£i √≠ch v√† gi√° tr·ªã m√† kh√≥a h·ªçc mang l·∫°i cho h·ªçc vi√™n.
- T·ª± ƒë·ªông h·ªó tr·ª£ t∆∞ v·∫•n, ch·ªß ƒë·ªông h·ªèi kh√°ch h√†ng ƒë·ªÉ hi·ªÉu v·ªÅ nhu c·∫ßu c·ªßa kh√°ch h√†ng, thuy·∫øt ph·ª•c kh√°ch h√†ng h·ªçc c√°c kh√≥a h·ªçc combo. 
- T·ª± ƒë·ªông h·ªó tr·ª£ t∆∞ v·∫•n kh√¥ng ƒë·ªÉ kh√°ch ch·ªß ƒë·ªông tr·∫£ l·ªùi tr∆∞·ªõc, bot ph·∫£i ch·ªß ƒë·ªông tr·∫£ l·ªùi tr∆∞·ªõc ƒë·ªÉ hi·ªÉu ƒë∆∞·ª£c kh√°ch h√†ng c·∫ßn t√¨m kh√≥a h·ªçc hay combo g√¨ ƒë·ªÉ t∆∞ v·∫•n, d·ªØ li·ªáu ƒë∆∞·ª£c l·∫•y trong 'course_data_edurich'. 
- T∆∞ v·∫•n v√† h∆∞·ªõng kh√°ch h√†ng t·ªõi c√°c kh√≥a h·ªçc combo (n√≥ bao g·ªìm nhi·ªÅu c√°c kh√≥a h·ªçc ƒë∆°n l·∫ª) ƒë·ªÉ kh√°ch h√†ng l·ª±a ch·ªçn v√¨ c√°c kh√≥a h·ªçc combo r·∫ª h∆°n nhi·ªÅu so v·ªõi c√°c kh√≥a ƒë∆°n l·∫ª, d·ªØ li·ªáu ƒë∆∞·ª£c l·∫•y trong 'course_data_edurich'. 


"""

        response = bedrock.invoke_model_with_response_stream(
            modelId=CLAUDE_SONNET_3_5,
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2000,
                "messages": [{"role": "user", "content": system_prompt}],
                "temperature": 0.3
            })
        )

        full_response = []
        for event in response.get('body', []):
            if chunk := event.get('chunk'):
                try:
                    data = json.loads(chunk['bytes'].decode())
                    if text := data.get('delta', {}).get('text', ''):
                        for char in text:
                            full_response.append(char)
                            yield char
                            time.sleep(0.001)
                except Exception as e:
                    logger.error(f"Chunk error: {str(e)}")
                    continue

        answer_text = "".join(full_response)

        # H√†m ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan gi·ªØa citation v√† c√¢u tr·∫£ l·ªùi
        def is_relevant(citation_text, answer_text, threshold=0.3):
            cit_words = set(citation_text.lower().split())
            ans_words = set(answer_text.lower().split())
            if not cit_words:
                return False
            ratio = len(cit_words.intersection(ans_words)) / len(cit_words)
            return ratio >= threshold

        # L·ªçc c√°c citation c√≥ li√™n quan d·ª±a tr√™n citation 'content'
        relevant_citations = [cite for cite in citations if is_relevant(cite['content'], answer_text)]
        
        if relevant_citations:
            # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ cho ph·∫ßn tr√≠ch d·∫´n
            yield "\n\n__üìå Tr√≠ch d·∫´n li√™n quan:__\n"
            for cite in relevant_citations:
                # N·∫øu c·∫ßn, b·∫°n c√≥ th·ªÉ r√∫t g·ªçn ph·∫ßn n·ªôi dung hi·ªÉn th·ªã ban ƒë·∫ßu
                full_text = cite.get('full', cite['content'])
                truncated = full_text
                if len(full_text) > 300:
                    truncated = full_text[:300] + "..."
                # T·∫°o toggle s·ª≠ d·ª•ng th·∫ª <details> v√† <summary>
                toggle_html = f"""
<details>
  <summary><strong>{cite['source']}</strong> (nh·∫•n ƒë·ªÉ xem chi ti·∫øt)</summary>
  <p>{full_text}</p>    
</details>
"""
                yield toggle_html

    except Exception as e:
        logger.error(f"General error: {str(e)}")
        yield "‚õî ƒê√£ x·∫£y ra l·ªói nghi√™m tr·ªçng. Vui l√≤ng th·ª≠ l·∫°i sau."
